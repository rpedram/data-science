{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtHfkeyoaErOMfxHYm0+VM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rpedram/data-science/blob/main/my_library_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Rakesh Function library\n",
        "These a list of my repeatedly used data science function that can either be called for directly after importing module or copied pasted in your code.\n",
        "Remember to import relevant the libraries in main pgme\n"
      ],
      "metadata": {
        "id": "ZzzLkEyKXFCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Data Loading\n",
        "Data loading for google drive ans summary of df. Initial eyeballing"
      ],
      "metadata": {
        "id": "VvABISB_XM8T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pnmwq0jgUelO"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"rak_analysis_lib.py\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1sUA0ZwTuO1xkwnyOhro_Kl-UWf0NZ04X\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Test and trouble shoot module import\n",
        "def test_function():\n",
        "    return \"Hello your module rak_analysis_lib! has been succesfully imported\"\n",
        "# --- EDA Functions ---\n",
        "# Load my df from google drive\n",
        "def load_data(data_path):\n",
        "    \"\"\"Loads data from a CSV or Excel file.\"\"\"\n",
        "    data_path='/content/drive/My Drive/Dataset/'+ data_path\n",
        "    if data_path.endswith(\".csv\"):\n",
        "        return pd.read_csv(data_path)\n",
        "    elif data_path.endswith(\".xlsx\"):\n",
        "        return pd.read_excel(data_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Supported formats: CSV, Excel\")\n",
        "# saves my df to google drive\n",
        "def save_data(df,filename):\n",
        "    \"\"\"Saves data from a CSV or Excel file to my google drive\"\"\"\n",
        "    data_path='/content/drive/My Drive/Dataset/'+ filename\n",
        "    if data_path.endswith(\".csv\"):\n",
        "        df.to_csv(data_path, index=False)\n",
        "    elif data_path.endswith(\".xlsx\"):\n",
        "        df.to_excel(data_path, index=False)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Supported formats: CSV, Excel\")\n",
        "\n",
        "def data_summary(data):\n",
        "    \"\"\"Provides descriptive statistics for a DataFrame.\"\"\"\n",
        "    print(data.describe(include=\"all\"))\n",
        "    print(data.isna().sum())  # Check for missing values\n",
        "    print(data.head()) # Print the head\n",
        "    print(data.tail()) # Print tail of data\n",
        "\n",
        "\"\"\"## Descriptive deep Statistical study\n",
        "Experimented with tabulating the numeric data into tables for skewness indec and normality using Wilk-Shapiro\n",
        "\"\"\"\n",
        "\n",
        "from scipy.stats import shapiro\n",
        "from scipy.stats import skew\n",
        "# Tail classification of data\n",
        "def classify_tail(df):\n",
        "    \"\"\"\n",
        "    Computes tail index and classifies tail behavior for each numeric column in the DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        None (Prints the table with column name, Tail Index, and Classification).\n",
        "    \"\"\"\n",
        "    print(f\"{'Column Name':<20} {'Tail Index':<12} {'Classification':<15}\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    for col in df.columns:\n",
        "        if pd.api.types.is_numeric_dtype(df[col]):\n",
        "            data = df[col]\n",
        "            tail_index = skew(data, bias=False)  # Using sample skewness (unbiased)\n",
        "\n",
        "            if tail_index < -1:\n",
        "                classification = \"Heavy Tail\"\n",
        "            elif -1 <= tail_index <= 1:\n",
        "                classification = \"Stable\"\n",
        "            else:\n",
        "                classification = \"Light Tail\"\n",
        "        else:\n",
        "            tail_index = \"N/A\"\n",
        "            classification = \"Not applicable\"\n",
        "\n",
        "        print(f\"{col:<20} {tail_index:<12} {classification:<15}\")\n",
        "\n",
        "# Function to check normality\n",
        "def check_normality(df):\n",
        "    \"\"\"\n",
        "    Computes normality test (Shapiro-Wilk) for numeric columns in the DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        None (Prints the table with column name, Shapiro index, and classification).\n",
        "    \"\"\"\n",
        "    print(f\"{'Column Name':<20} {'Shapiro Index':<15} {'Classification':<15}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for col in df.columns:\n",
        "        if pd.api.types.is_numeric_dtype(df[col]):\n",
        "            _, p_value = shapiro(df[col])\n",
        "\n",
        "            if p_value < 0.05:\n",
        "                classification = \"Not Normal\"\n",
        "            else:\n",
        "                classification = \"Normal\"\n",
        "\n",
        "            print(f\"{col:<20} {p_value:.4f} {classification:<15}\")\n",
        "        else:\n",
        "            print(f\"{col:<20} N/A {'Not applicable':<15}\")\n",
        "    print('Recheck for non normal data before ttest/ ANOVA after outlier')\n",
        "\n",
        "\"\"\"## Data Transformation Function\n",
        "This codeblock has all outlier treatment and imputation function\n",
        "\"\"\"\n",
        "\n",
        "def handle_missing_values(data, method=\"mean\"):\n",
        "    \"\"\"Handles missing values using specified method ('mean', 'median', or 'drop').\"\"\"\n",
        "    if method == \"mean\":\n",
        "        return data.fillna(data.mean())\n",
        "    elif method == \"median\":\n",
        "        return data.fillna(data.median())\n",
        "    elif method == \"drop\":\n",
        "        return data.dropna()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid method. Supported methods: 'mean', 'median', 'drop'\")\n",
        "# Outlier treatment\n",
        "\"\"\"\n",
        "    Detects outliers in a numerical column using IQR and removes or winsorizes them.\n",
        "\n",
        "    Args: data frame\n",
        "\n",
        "    Returns:        The cleaned df with outliers removed or winsorized.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "def treat_outliers_clip(df,exclude_list):\n",
        "\n",
        "    # Identify numerical columns for analysis\n",
        "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.difference(exclude_list)\n",
        "\n",
        "\n",
        "    # Create a function for outlier detection and treatment\n",
        "    def detect_outliers(column):\n",
        "\n",
        "\n",
        "        # Calculate IQR and quantiles\n",
        "        q1 = column.quantile(0.25)\n",
        "        q3 = column.quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "\n",
        "        # Identify potential outliers (values beyond 1.5 IQR from quartiles)\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "        outliers = column[(column < lower_bound) | (column > upper_bound)]\n",
        "        #Clip data to upper and lower bounds\n",
        "        cleaned_column = column.clip(lower_bound, upper_bound,axis=0)\n",
        "\n",
        "        return cleaned_column\n",
        "    clean_df=df.copy()\n",
        "# Iterate through numerical columns and visualize distributions\n",
        "    for column in numerical_columns:\n",
        "        print(f\"\\nAnalyzing column: {column}\")\n",
        "        clean_df[column] = detect_outliers(df[column])\n",
        "    return clean_df\n",
        "\n",
        "# Winsorize function\n",
        "from scipy.stats import mstats\n",
        "\n",
        "def winsorize_numeric_columns(df, exclude_col=None, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Winsorizes numeric columns in the DataFrame (excluding specified columns).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame.\n",
        "        exclude_col (list or None): List of column names to exclude (optional).\n",
        "        alpha (float): Winsorization parameter (default is 0.05).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Cleaned DataFrame with winsorized numeric columns.\n",
        "    \"\"\"\n",
        "    if exclude_col is None:\n",
        "        exclude_col = []  # Initialize as an empty list if not provided\n",
        "\n",
        "    # Select numeric columns excluding specified ones\n",
        "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.difference(exclude_col)\n",
        "\n",
        "    # Apply winsorization to selected numeric columns\n",
        "    for col in numeric_cols:\n",
        "        df[col] = mstats.winsorize(df[col], limits=[alpha, alpha])\n",
        "\n",
        "    return df\n",
        "\n",
        "\"\"\"## Data Visualisation Function\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Box Plot and histogrram Side by Side takes two df as parameters and a exclude list\n",
        "def compare_distributions(data, df_cleaned,exclude_list=None):\n",
        "\n",
        "    numerical_columns = data.select_dtypes(include=['int64', 'float64']).columns.difference(exclude_list)\n",
        "\n",
        "    for col in numerical_columns:\n",
        "        print(f\"\\nAnalyzing column: {col}\")\n",
        "\n",
        "        # Set Plot size\n",
        "        plt.figure(figsize=(15, 8))\n",
        "\n",
        "        # Original DataFrame Boxplot\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.boxplot(data=data[col])\n",
        "        plt.title(f'Original DataFrame - {col}')\n",
        "\n",
        "        # Cleaned DataFrame Boxplot\n",
        "        plt.subplot(1, 2, 2)\n",
        "        sns.boxplot(data=df_cleaned[col])\n",
        "        plt.title(f'Cleaned DataFrame - {col}')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Separate Figure for Histograms\n",
        "        plt.figure(figsize=(15, 8))\n",
        "\n",
        "        # Original DataFrame Histogram\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.histplot(data=data[col], kde=True)\n",
        "        plt.title(f'Original DataFrame - {col}')\n",
        "\n",
        "        # Cleaned DataFrame Histogram\n",
        "        plt.subplot(1, 2, 2)\n",
        "        sns.histplot(data=df_cleaned[col], kde=True)\n",
        "        plt.title(f'Cleaned DataFrame - {col}')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Plot correlation matrix\n",
        "def plot_corr_matrix(df,predictors):\n",
        "\n",
        "    corr_matrix = df[predictors].corr()\n",
        "\n",
        "    # Heatmap visualization\n",
        "\n",
        "    # Increase size\n",
        "    plt.figure(figsize=(12, 8))  # Adjust width and height as desired\n",
        "\n",
        "    # Select a colormap:\n",
        "    cmap = sns.color_palette(\"coolwarm\", as_cmap=True)  # Try other palettes!\n",
        "\n",
        "    # Create the heatmap\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap=cmap)\n",
        "    plt.title('Correlation Heatmap Enhanced')\n",
        "    plt.show()\n",
        "    # Convert the correlation matrix to a DataFrame (if it isn't already)\n",
        "    if not isinstance(corr_matrix, pd.DataFrame):\n",
        "        corr_matrix = pd.DataFrame(corr_matrix)\n",
        "\n",
        "    # Export to Excel\n",
        "    corr_matrix.to_excel('/content/drive/My Drive/Dataset/' +'correlation_matrix.xlsx', index=True)\n",
        "    # 'index=True' will include the row and column labels\n",
        "\n",
        "\"\"\"## Machine Learning function\"\"\"\n",
        "\n",
        "# @title Import Libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def evaluate_model_regress(df, target_variable, feature_cols, ordinal_features, onehot_features, exclude_col,models):\n",
        "    X = df[feature_cols]\n",
        "    y = df[target_variable]\n",
        "    # Track metrics\n",
        "    mse_scores = []\n",
        "    r2_scores = []\n",
        "    # Train-test split using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    # Preprocessing Pipeline\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('oe_type', OrdinalEncoder(categories='auto'), ordinal_features),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False), onehot_features),\n",
        "            ('scaler', StandardScaler(), [col for col in feature_cols if col not in ordinal_features + onehot_features+ exclude_col])\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    # Build the final Pipeline\n",
        "    pipe = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', models)\n",
        "    ])\n",
        "\n",
        "    # Fit the pipeline\n",
        "    pipe.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = pipe.predict(X_test)\n",
        "\n",
        "    # Calculate mean squared error\n",
        "    # Calculate metrics\n",
        "    mse_scores.append(mean_squared_error(y_test, y_pred))\n",
        "    r2_scores.append(r2_score(y_test, y_pred))\n",
        "    # ... (Rest of the function for feature importances and plotting)\n",
        "\n",
        "    return mse_scores, r2_scores # Return MSE and predictions\n",
        "    # Bar chart of feature of importace if tree based algo choseen\n",
        "    # Feature importances (applicable to tree-based models)\n",
        "    if hasattr(models, 'feature_importances_'):\n",
        "      feature_importances = pd.Series(models.feature_importances_, index=feature_cols)\n",
        "      feature_importances.sort_values(ascending=False, inplace=True)\n",
        "\n",
        "      # Plot feature importances\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      sns.barplot(x=feature_importances.index, y=feature_importances.values)\n",
        "      plt.title(f\"Feature Importances: {models.__class__.__name__}\")\n",
        "      plt.xticks(rotation=45)\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "# @title Stratified fold Split with classifier algo- Under progress\n",
        "# Cant import stratified Kfold in colab version clash. Download file from github\n",
        "''''\n",
        "# Using K stratified fold\n",
        "import sklearn\n",
        "print(sklearn.__version__)\n",
        "!pip install scikit-learn-contrib-stratified-kfold==0.2\n",
        "\n",
        "from sklearn_contrib.stratified_kfold import KStratifiedFold\n",
        "\n",
        "\n",
        "def evaluate_model_classify(df, target_variable, feature_cols, ordinal_features, onehot_features, model):\n",
        "    X = df[feature_cols]\n",
        "    y = df[target_variable]\n",
        "\n",
        "    # KStratifiedFold for cross-validation\n",
        "    cv = KStratifiedFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Track metrics\n",
        "    mse_scores = []\n",
        "    r2_scores = []\n",
        "    accuracy_scores = []\n",
        "\n",
        "    for train_index, test_index in cv.split(X, y):\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        # Preprocessing Pipeline\n",
        "        preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('oe_type', OrdinalEncoder(categories='auto'), ordinal_features),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False), onehot_features),\n",
        "            ('scaler', StandardScaler(), [col for col in feature_cols if col not in ordinal_features + onehot_features])\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "        )\n",
        "    # Build the final Pipeline\n",
        "        pipe = Pipeline(steps=[\n",
        "          ('preprocessor', preprocessor),\n",
        "          ('model', model)\n",
        "        ])\n",
        "\n",
        "        # Fit the pipeline\n",
        "        pipe.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = pipe.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        mse_scores.append(mean_squared_error(y_test, y_pred))\n",
        "        r2_scores.append(r2_score(y_test, y_pred))\n",
        "\n",
        "    return mse_scores, r2_scores, y_pred\n",
        "    '''\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Frequently used transformation"
      ],
      "metadata": {
        "id": "Zx12R3rTkoXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function returns numeric column list minus the excluded column. Very quick from reselecting data fields\n",
        "\n",
        "def get_numeric_columns(df, exclude_columns=None):\n",
        "    \"\"\"\n",
        "    Returns a list of numeric column names from the DataFrame, excluding specified columns.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame.\n",
        "        exclude_columns (list or None): List of column names to exclude (optional).\n",
        "\n",
        "    Returns:\n",
        "        list: List of numeric column names.\n",
        "    \"\"\"\n",
        "    if exclude_columns is None:\n",
        "        exclude_columns = []  # Initialize as an empty list if not provided\n",
        "\n",
        "    numeric_columns = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]\n",
        "    filtered_numeric_columns = [col for col in numeric_columns if col not in exclude_columns]\n",
        "\n",
        "    return filtered_numeric_columns\n",
        "\n",
        "# Function converts columns to snake case\n",
        "import re\n",
        "\n",
        "def convert_columns_to_snake_case(df):\n",
        "    \"\"\"Converts all column names in a DataFrame to snake_case.\n",
        "\n",
        "    Args:\n",
        "         DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        The DataFrame with modified column names.\n",
        "    \"\"\"\n",
        "    # Modify depending on what you want to delete\n",
        "    def to_snake_case(name):\n",
        "        name = re.sub(r'[^\\w_]', '', name)  # Remove special characters except alphanumeric, underscore\n",
        "        name = name.replace(' ', '_')  # Replace spaces with underscore\n",
        "        return name.lower()  # Convert to lowercase\n",
        "\n",
        "    df.rename(columns=to_snake_case, inplace=True)\n",
        "    return df"
      ],
      "metadata": {
        "id": "4O25pQV4kiQZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}